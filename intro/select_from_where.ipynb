{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3a267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oktavianu/miniforge3/envs/data-analysis/lib/python3.11/site-packages/google/cloud/bigquery/__init__.py:31: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import get_distribution\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d758b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oktavianu/miniforge3/envs/data-analysis/lib/python3.11/site-packages/google/auth/_default.py:70: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. We recommend you rerun `gcloud auth application-default login` and make sure a quota project is added. Or you can use service accounts instead. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# create clinet object\n",
    "client = bigquery.Client(\"intsql-2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc54f3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct ref to openaq dataset\n",
    "dataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7334db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Api req, fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b43ff1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_air_quality\n"
     ]
    }
   ],
   "source": [
    "# List all tables in openaq dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "# and print all tables' name in the dataset\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a9f69e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7m/y8yqbwcj4_s9pbq64p2cdkw00000gn/T/ipykernel_6915/2317965706.py:8: UserWarning: Cannot use bqstorage_client if max_results is set, reverting to fetching data with the tabledata.list endpoint.\n",
      "  client.list_rows(table, max_results=5).to_dataframe()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>city</th>\n",
       "      <th>country</th>\n",
       "      <th>pollutant</th>\n",
       "      <th>value</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>unit</th>\n",
       "      <th>source_name</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>averaged_over_in_hours</th>\n",
       "      <th>location_geom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Borówiec, ul. Drapałka</td>\n",
       "      <td>Borówiec</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.85217</td>\n",
       "      <td>2022-04-28 07:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.276794</td>\n",
       "      <td>17.074114</td>\n",
       "      <td>POINT(52.276794 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kraków, ul. Bulwarowa</td>\n",
       "      <td>Kraków</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.91284</td>\n",
       "      <td>2022-04-27 23:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.069308</td>\n",
       "      <td>20.053492</td>\n",
       "      <td>POINT(50.069308 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Płock, ul. Reja</td>\n",
       "      <td>Płock</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>1.41000</td>\n",
       "      <td>2022-03-30 04:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.550938</td>\n",
       "      <td>19.709791</td>\n",
       "      <td>POINT(52.550938 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elbląg, ul. Bażyńskiego</td>\n",
       "      <td>Elbląg</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.33607</td>\n",
       "      <td>2022-05-03 13:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>54.167847</td>\n",
       "      <td>19.410942</td>\n",
       "      <td>POINT(54.167847 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Piastów, ul. Pułaskiego</td>\n",
       "      <td>Piastów</td>\n",
       "      <td>PL</td>\n",
       "      <td>bc</td>\n",
       "      <td>0.51000</td>\n",
       "      <td>2022-05-11 05:00:00+00:00</td>\n",
       "      <td>µg/m³</td>\n",
       "      <td>GIOS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>52.191728</td>\n",
       "      <td>20.837489</td>\n",
       "      <td>POINT(52.191728 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  location      city country pollutant    value  \\\n",
       "0   Borówiec, ul. Drapałka  Borówiec      PL        bc  0.85217   \n",
       "1    Kraków, ul. Bulwarowa    Kraków      PL        bc  0.91284   \n",
       "2          Płock, ul. Reja     Płock      PL        bc  1.41000   \n",
       "3  Elbląg, ul. Bażyńskiego    Elbląg      PL        bc  0.33607   \n",
       "4  Piastów, ul. Pułaskiego   Piastów      PL        bc  0.51000   \n",
       "\n",
       "                  timestamp   unit source_name  latitude  longitude  \\\n",
       "0 2022-04-28 07:00:00+00:00  µg/m³        GIOS       1.0  52.276794   \n",
       "1 2022-04-27 23:00:00+00:00  µg/m³        GIOS       1.0  50.069308   \n",
       "2 2022-03-30 04:00:00+00:00  µg/m³        GIOS       1.0  52.550938   \n",
       "3 2022-05-03 13:00:00+00:00  µg/m³        GIOS       1.0  54.167847   \n",
       "4 2022-05-11 05:00:00+00:00  µg/m³        GIOS       1.0  52.191728   \n",
       "\n",
       "   averaged_over_in_hours       location_geom  \n",
       "0               17.074114  POINT(52.276794 1)  \n",
       "1               20.053492  POINT(50.069308 1)  \n",
       "2               19.709791  POINT(52.550938 1)  \n",
       "3               19.410942  POINT(54.167847 1)  \n",
       "4               20.837489  POINT(52.191728 1)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct a ref to the table \"global_air_quality\"\n",
    "table_ref = dataset_ref.table(\"global_air_quality\")\n",
    "\n",
    "# api req, fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# preview the first five lines\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1c6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select all the items from the \"city\" column where the \"country\" column is 'US\n",
    "query = \"\"\" \n",
    "        SELECT city\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15318abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the query\n",
    "query_job = client.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9043fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API request - run the query, and return a pandas DataFrame\n",
    "us_cities = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cb398a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "Phoenix-Mesa-Scottsdale                     39414\n",
       "Los Angeles-Long Beach-Santa Ana            27479\n",
       "Riverside-San Bernardino-Ontario            26887\n",
       "New York-Northern New Jersey-Long Island    25417\n",
       "San Francisco-Oakland-Fremont               22710\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_cities.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44aa3fde",
   "metadata": {},
   "source": [
    " multiple columns\n",
    "```python\n",
    "query = \"\"\"\n",
    "        SELECT city, country\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\"\n",
    "```\n",
    "or selecting all columns\n",
    "```python\n",
    "query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced11a45",
   "metadata": {},
   "source": [
    "#### Working with big datasets\n",
    "It is better to know how to estimate the query's size. Her is how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e627acd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This query will process 716584537 bytes.\n"
     ]
    }
   ],
   "source": [
    "# Query to get the score column from every row where the type column has value \"job\"\n",
    "query = \"\"\" \n",
    "        SELECT score, title\n",
    "        FROM `bigquery-public-data.hacker_news.full`\n",
    "        WHERE type = \"job\"\n",
    "        \"\"\"\n",
    "\n",
    "# Create a QueryJobConfig object to estimate size of query without running it\n",
    "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "\n",
    "# API request - dry run query to estimate costs\n",
    "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
    "\n",
    "print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f3e11",
   "metadata": {},
   "source": [
    "#### Why we better know the size of a query before running it?\n",
    "1. Cost Control (Most Significant Factor)\n",
    "2. Performance and Efficiency\n",
    "3. reventing Full Table Scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16609488",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "500 Query exceeded limit for bytes billed: 1000000. 717225984 or higher required.\n\n(job ID: 0f517f95-0ac0-4c3a-b2f6-8fef13e44c33)\n\n             -----Query Job SQL Follows-----             \n\n    |    .    |    .    |    .    |    .    |    .    |\n   1: \n   2:        SELECT score, title\n   3:        FROM `bigquery-public-data.hacker_news.full`\n   4:        WHERE type = \"job\"\n   5:        \n    |    .    |    .    |    .    |    .    |    .    |",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m safe_query_job = client.query(query, job_config=safe_config)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# API request - try to run the query, and return a pandas DataFrame\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43msafe_query_job\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/data-analysis/lib/python3.11/site-packages/google/cloud/bigquery/job.py:3401\u001b[39m, in \u001b[36mQueryJob.to_dataframe\u001b[39m\u001b[34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, date_as_object)\u001b[39m\n\u001b[32m   3340\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mto_dataframe\u001b[39m(\n\u001b[32m   3341\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   3342\u001b[39m     bqstorage_client=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3346\u001b[39m     date_as_object=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   3347\u001b[39m ):\n\u001b[32m   3348\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[32m   3349\u001b[39m \n\u001b[32m   3350\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3399\u001b[39m \u001b[33;03m        ValueError: If the `pandas` library cannot be imported.\u001b[39;00m\n\u001b[32m   3400\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3401\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to_dataframe(\n\u001b[32m   3402\u001b[39m         bqstorage_client=bqstorage_client,\n\u001b[32m   3403\u001b[39m         dtypes=dtypes,\n\u001b[32m   3404\u001b[39m         progress_bar_type=progress_bar_type,\n\u001b[32m   3405\u001b[39m         create_bqstorage_client=create_bqstorage_client,\n\u001b[32m   3406\u001b[39m         date_as_object=date_as_object,\n\u001b[32m   3407\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/data-analysis/lib/python3.11/site-packages/google/cloud/bigquery/job.py:3230\u001b[39m, in \u001b[36mQueryJob.result\u001b[39m\u001b[34m(self, page_size, max_results, retry, timeout, start_index)\u001b[39m\n\u001b[32m   3193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Start the job and wait for it to complete and get the result.\u001b[39;00m\n\u001b[32m   3194\u001b[39m \n\u001b[32m   3195\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3227\u001b[39m \u001b[33;03m        If the job did not complete in the given timeout.\u001b[39;00m\n\u001b[32m   3228\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3229\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3230\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mQueryJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3232\u001b[39m     \u001b[38;5;66;03m# Return an iterator instead of returning the job.\u001b[39;00m\n\u001b[32m   3233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._query_results:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/data-analysis/lib/python3.11/site-packages/google/cloud/bigquery/job.py:835\u001b[39m, in \u001b[36m_AsyncJob.result\u001b[39m\u001b[34m(self, retry, timeout)\u001b[39m\n\u001b[32m    833\u001b[39m     \u001b[38;5;28mself\u001b[39m._begin(retry=retry, timeout=timeout)\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m# TODO: modify PollingFuture so it can pass a retry argument to done().\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m835\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_AsyncJob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/data-analysis/lib/python3.11/site-packages/google/api_core/future/polling.py:135\u001b[39m, in \u001b[36mPollingFuture.result\u001b[39m\u001b[34m(self, timeout, retry)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m._blocking_poll(timeout=timeout, **kwargs)\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    133\u001b[39m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[32m    134\u001b[39m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[31mInternalServerError\u001b[39m: 500 Query exceeded limit for bytes billed: 1000000. 717225984 or higher required.\n\n(job ID: 0f517f95-0ac0-4c3a-b2f6-8fef13e44c33)\n\n             -----Query Job SQL Follows-----             \n\n    |    .    |    .    |    .    |    .    |    .    |\n   1: \n   2:        SELECT score, title\n   3:        FROM `bigquery-public-data.hacker_news.full`\n   4:        WHERE type = \"job\"\n   5:        \n    |    .    |    .    |    .    |    .    |    .    |"
     ]
    }
   ],
   "source": [
    "# specify a parameter when running the query to limit how much data you are willing to scan\n",
    "# only run query if it is less than 1 mb\n",
    "ONE_MB = 1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 MB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "safe_query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10237e",
   "metadata": {},
   "source": [
    "The query was cancelled, because the limit of 1 MB was exceeded. However, we can increase the limit to run the query successfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6dd87e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(1.6455829151592951)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_gb = 1000*1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=one_gb)\n",
    "\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "job_post_scores = safe_query_job.to_dataframe()\n",
    "\n",
    "# Print average score for job posts\n",
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b2268",
   "metadata": {},
   "source": [
    "Which countries have reported pollution levels in units of \"ppm\"? In the code cell below, set first_query to an SQL query that pulls the appropriate entries from the country column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4044fe85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country\n",
      "0      AR\n",
      "1      AR\n",
      "2      AR\n",
      "3      AR\n",
      "4      CO\n"
     ]
    }
   ],
   "source": [
    "ppm_query = \"\"\"\n",
    "            SELECT country\n",
    "            FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "            WHERE unit = \"ppm\"\n",
    "            \"\"\"\n",
    "\n",
    "# Set up the query (cancel the query if it would use too much of \n",
    "# the quota, with the limit set to 10 GB)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "ppm_query_job = client.query(ppm_query, job_config=safe_config)\n",
    "\n",
    "\n",
    "# API request - run the query, and return a pandas DataFrame\n",
    "results = ppm_query_job.to_dataframe()\n",
    "\n",
    "# View top few rows of results\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df2a0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
